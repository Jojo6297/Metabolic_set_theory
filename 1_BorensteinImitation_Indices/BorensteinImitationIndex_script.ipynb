{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import glob\n",
    "import natsort\n",
    "import math\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "input_path = os.path.join(parent_dir, \"_ModelsAndNetworks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Actinomyces_odontolyticus_ATCC_17982',\n",
       " 'Alistipes_putredinis_DSM_17216',\n",
       " 'Anaerococcus_hydrogenalis_DSM_7454',\n",
       " 'Anaerofustis_stercorihominis_DSM_17244',\n",
       " 'Anaerostipes_caccae_DSM_14662',\n",
       " 'Anaerotruncus_colihominis_DSM_17241',\n",
       " 'Bacteroides_caccae_ATCC_43185',\n",
       " 'Bacteroides_cellulosilyticus_DSM_14838',\n",
       " 'Bacteroides_coprophilus_DSM_18228',\n",
       " 'Bacteroides_dorei_DSM_17855']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Edge_filenames = natsort.natsorted(glob.glob(input_path + \"/*Edgelist.txt\"))\n",
    "Species = ['_'.join(j.split('.')[0].split('_')[:-1]) for j in [i.split('\\\\')[-1] for i in Edge_filenames]]\n",
    "\n",
    "\n",
    "Met_adjs = []\n",
    "for i in range(len(Species)):\n",
    "    M_edge = pd.read_table(Edge_filenames[i], header = None)\n",
    "    M_edge.columns = ['source', 'target']\n",
    "    M_network = nx.from_pandas_edgelist(M_edge, create_using=nx.DiGraph)\n",
    "    M_ad = nx.to_pandas_adjacency(M_network)\n",
    "    Met_adjs.append(M_ad)\n",
    "\n",
    "Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove glycans\n",
    "glc_Met_adjs = []\n",
    "glc_Met_IDs = []\n",
    "for adj in Met_adjs:\n",
    "    \n",
    "    ID = list(adj.columns)\n",
    "    G = nx.from_pandas_adjacency(adj, create_using= nx.DiGraph)\n",
    "\n",
    "    glcNodes = [i for i in ID if 'MGlcn' in i]\n",
    "\n",
    "    glc_N = copy.deepcopy(G)\n",
    "    glc_N.remove_nodes_from(glcNodes)\n",
    "\n",
    "    glc_adj = nx.to_pandas_adjacency(glc_N)\n",
    "    glc_ID = list(glc_adj.columns)\n",
    "\n",
    "    glc_Met_adjs.append(glc_adj)\n",
    "    glc_Met_IDs.append(glc_ID)\n",
    "    \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Find and remove currency metabolites\n",
    "perc = 0.03\n",
    "\n",
    "curr_Met_adjs = []\n",
    "curr_Met_IDs = []\n",
    "for adj0 in glc_Met_adjs:\n",
    "    ID = list(adj0.columns)\n",
    "    top_n_nodes = math.ceil(perc*len(ID))\n",
    "\n",
    "    glc_G = nx.from_pandas_adjacency(adj0, create_using= nx.DiGraph)\n",
    "    SortedNodes_Degree = sorted(dict(glc_G.degree(glc_G.nodes())).items(), key=lambda x:x[1], reverse=True)\n",
    "    SortedNodes = [i[0] for i in SortedNodes_Degree]\n",
    "\n",
    "    #remove\n",
    "    curr_N = copy.deepcopy(glc_G)\n",
    "    curr_N.remove_nodes_from(SortedNodes[:top_n_nodes])\n",
    "\n",
    "    curr_adj = nx.to_pandas_adjacency(curr_N)\n",
    "    curr_ID = list(curr_adj.columns)\n",
    "\n",
    "    curr_Met_adjs.append(curr_adj)\n",
    "    curr_Met_IDs.append(curr_ID)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Find and remove small weekly connected components\n",
    "threshold = 10\n",
    "\n",
    "wcc_Met_adjs = []\n",
    "wcc_Met_IDs = []\n",
    "for adj1 in curr_Met_adjs:\n",
    "    curr_G = nx.from_pandas_adjacency(adj1, create_using= nx.DiGraph)\n",
    "\n",
    "    sorted_CC_list = [sorted(list(c)) for c in sorted(nx.weakly_connected_components(curr_G), key=len, reverse=True)]\n",
    "    removenodes = [i for i in sorted_CC_list if len(i) <= threshold]\n",
    "    removenodes = [val for sublist in removenodes for val in sublist]\n",
    "\n",
    "    wcc_N = copy.deepcopy(curr_G)\n",
    "    wcc_N.remove_nodes_from(removenodes)\n",
    "\n",
    "    wcc_adj = nx.to_pandas_adjacency(wcc_N)\n",
    "    wcc_ID = list(wcc_adj.columns)\n",
    "\n",
    "    wcc_Met_adjs.append(wcc_adj)\n",
    "    wcc_Met_IDs.append(wcc_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export filtered adjacency networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{current_dir}/0_FilteredAdjacency', exist_ok=True)\n",
    "for m, fil_adj in enumerate(wcc_Met_adjs):\n",
    "    fil_adj.to_csv(f'{current_dir}/0_FilteredAdjacency/{Species[m]}_FilteredNetwork.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the seed set of each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeedSet_confidence_list = []\n",
    "SeedSet_list = []\n",
    "\n",
    "for adj_,ID in zip(wcc_Met_adjs, wcc_Met_IDs):\n",
    "\n",
    "    G_raph = nx.from_pandas_adjacency(adj_, create_using= nx.DiGraph)\n",
    "\n",
    "    sorted_SCCs_list = [sorted(list(c)) for c in sorted(nx.kosaraju_strongly_connected_components(G_raph), key=len, reverse=True)]\n",
    "\n",
    "    ## IF WANT TO FILTER THE LARGE SCCs FROM CONSIDERATION AS SEED SETS\n",
    "    #sorted_SCCs_list = [i for i in sorted_SCCs_list if len(i) <= 5]\n",
    "\n",
    "    SizeOfSCC = [len(i) for i in sorted_SCCs_list]\n",
    "    edgelist_ = G_raph.edges()\n",
    "\n",
    "    source_seed = []\n",
    "    source_seed_dict = {}\n",
    "    for nodelist__ in sorted_SCCs_list:\n",
    "        for edges in edgelist_:\n",
    "            count = 0\n",
    "            if (edges[0] not in nodelist__ and edges[1] in nodelist__):\n",
    "                count += 1\n",
    "                break\n",
    "        if count == 0:\n",
    "            source_seed.append(nodelist__)\n",
    "\n",
    "    n_source_seed = []\n",
    "    for sSeed in source_seed:\n",
    "        temp = 0\n",
    "        for edge__s in edgelist_:\n",
    "            for i in sSeed:\n",
    "                if ((i, edge__s[1]) in edgelist_) & (edge__s[1] not in sSeed):\n",
    "                    temp += 1\n",
    "        if temp > 0:\n",
    "            n_source_seed.append(sSeed)\n",
    "\n",
    "    SeedSet_list.append(n_source_seed)\n",
    "    \n",
    "    seed_confidence = [[(i, 1/len(item)) for i in item] for item in n_source_seed]\n",
    "    Flat_seed_confidence = [val for sublist in seed_confidence for val in sublist]            \n",
    "    SeedSet_confidence_list.append(Flat_seed_confidence)\n",
    "\n",
    "Species_compound_dict = dict(zip(Species, SeedSet_confidence_list))\n",
    "Species_set_dict = dict(zip(Species, SeedSet_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export seed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{current_dir}/1_SpeciesMetaLayers', exist_ok=True)\n",
    "Seed_layer = pd.DataFrame([[i[0] for i in v] for k,v in Species_compound_dict.items()], index=Species).T\n",
    "Seed_layer.to_csv(f'{current_dir}/1_SpeciesMetaLayers/B-I_S.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and export competitive indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{current_dir}/2_Indices', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompIndexMatrixW = pd.DataFrame(np.zeros((len(Species), len(Species)), dtype=float), columns= list(Species_compound_dict.keys()))\n",
    "CompIndexMatrixW.index = CompIndexMatrixW.columns\n",
    "for speci1 in Species_compound_dict.keys():\n",
    "    for speci2 in Species_compound_dict.keys():\n",
    "\n",
    "        comp1 = [i[0] for i in Species_compound_dict[speci1]]\n",
    "        comp2 = [i[0] for i in Species_compound_dict[speci2]]\n",
    "        intersect = list(set(comp1) & set(comp2))\n",
    "\n",
    "        # with weights\n",
    "        numerW = np.sum([item[1] for item in Species_compound_dict[speci1] if item[0] in intersect])\n",
    "        denomW = np.sum([item[1] for item in Species_compound_dict[speci1]])\n",
    "        \n",
    "        CI_W = numerW/denomW\n",
    "        CompIndexMatrixW.loc[speci1, speci2] = CI_W\n",
    "\n",
    "CompIndexMatrixW.to_csv(f'{current_dir}/2_Indices/BorensteinImitate_Weighted_CI_SS_Curr{perc}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and export the product set of each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = {}\n",
    "count_0 = 0\n",
    "for speci in Species_compound_dict.keys():\n",
    "\n",
    "    SS = [i[0] for i in Species_compound_dict[speci]]\n",
    "    product_nodes = list(set(wcc_Met_IDs[count_0]) - set(SS))\n",
    "\n",
    "    products[speci] = [i for i in product_nodes] # if '_c' in i]\n",
    "    count_0 += 1\n",
    "\n",
    "Product_layer = pd.DataFrame([v for k,v in products.items()], index = Species).T\n",
    "Product_layer.to_csv(f'{current_dir}/1_SpeciesMetaLayers/B-I_P.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and export synergistic indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SynIndexMatrixW = pd.DataFrame(np.zeros((len(Species), len(Species)), dtype=float), columns= list(Species_compound_dict.keys()))\n",
    "SynIndexMatrixW.index = SynIndexMatrixW.columns\n",
    "for speci1 in Species_compound_dict.keys():\n",
    "    for speci2 in Species_compound_dict.keys():\n",
    "        if speci1 != speci2:\n",
    "\n",
    "            comp1 = [i[0] for i in Species_compound_dict[speci1]]\n",
    "            intersect_ = list(set(comp1) & set(products[speci2]))\n",
    "            \n",
    "            # with weights\n",
    "            numerW = np.sum([item[1] for item in Species_compound_dict[speci1] if item[0] in intersect_])\n",
    "            denomW = np.sum([item[1] for item in Species_compound_dict[speci1]])\n",
    "\n",
    "            SI_W = numerW/denomW\n",
    "\n",
    "            # print(f'{speci1} -> {speci2}: SI = {SI}')\n",
    "            # rows should be speci1 and columns speci2\n",
    "            SynIndexMatrixW.loc[speci1, speci2] = SI_W\n",
    "\n",
    "SynIndexMatrixW.to_csv(f'{current_dir}/2_Indices/BorensteinImitate_Weighted_SI_PS_Curr{perc}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbiomE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
