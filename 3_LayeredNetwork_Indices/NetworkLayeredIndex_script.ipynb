{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import glob\n",
    "import natsort\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "input_path = os.path.join(parent_dir, \"_ModelsAndNetworks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Actinomyces_odontolyticus_ATCC_17982',\n",
       " 'Alistipes_putredinis_DSM_17216',\n",
       " 'Anaerococcus_hydrogenalis_DSM_7454',\n",
       " 'Anaerofustis_stercorihominis_DSM_17244',\n",
       " 'Anaerostipes_caccae_DSM_14662',\n",
       " 'Anaerotruncus_colihominis_DSM_17241',\n",
       " 'Bacteroides_caccae_ATCC_43185',\n",
       " 'Bacteroides_cellulosilyticus_DSM_14838',\n",
       " 'Bacteroides_coprophilus_DSM_18228',\n",
       " 'Bacteroides_dorei_DSM_17855']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj_filenames = natsort.natsorted(glob.glob(input_path + \"/*Adjacency.csv\"))\n",
    "Biomass_filenames = natsort.natsorted(glob.glob(input_path + \"/*biomassReaction.txt\"))\n",
    "Species = ['_'.join(j.split('_')[:-1]) for j in [i.split('\\\\')[-1] for i in Adj_filenames]]\n",
    "\n",
    "Met_adjs = []\n",
    "Met_IDs = []\n",
    "BiomassRxns = []\n",
    "for i in range(len(Species)):\n",
    "    M_ad = pd.read_csv(Adj_filenames[i], index_col=0)\n",
    "    M_id = list(M_ad.columns)\n",
    "\n",
    "    B_rxn = pd.read_table(Biomass_filenames[i], header = None)[0].to_list()\n",
    "    B_rxn = [[j[1:-1] for j in item[1:-1].split(', ')] for item in B_rxn]\n",
    "\n",
    "    BiomassRxns.append(B_rxn)\n",
    "    Met_IDs.append(M_id)\n",
    "    Met_adjs.append(M_ad)\n",
    "\n",
    "Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove glycans\n",
    "glc_Met_adjs = []\n",
    "glc_Met_IDs = []\n",
    "for adj in Met_adjs:\n",
    "    \n",
    "    ID = list(adj.columns)\n",
    "    G = nx.from_pandas_adjacency(adj, create_using= nx.DiGraph)\n",
    "\n",
    "    glcNodes = [i for i in ID if 'MGlcn' in i]\n",
    "\n",
    "    glc_N = copy.deepcopy(G)\n",
    "    glc_N.remove_nodes_from(glcNodes)\n",
    "\n",
    "    glc_adj = nx.to_pandas_adjacency(glc_N)\n",
    "    glc_ID = list(glc_adj.columns)\n",
    "\n",
    "    glc_Met_adjs.append(glc_adj)\n",
    "    glc_Met_IDs.append(glc_ID)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Find and remove currency metabolites\n",
    "perc = 0.03\n",
    "\n",
    "curr_Met_adjs = []\n",
    "curr_Met_IDs = []\n",
    "for adj0 in glc_Met_adjs:\n",
    "    ID = list(adj0.columns)\n",
    "    top_n_nodes = math.ceil(perc*len(ID))\n",
    "\n",
    "    glc_G = nx.from_pandas_adjacency(adj0, create_using= nx.DiGraph)\n",
    "    SortedNodes_Degree = sorted(dict(glc_G.degree(glc_G.nodes())).items(), key=lambda x:x[1], reverse=True)\n",
    "    SortedNodes = [i[0] for i in SortedNodes_Degree]\n",
    "\n",
    "    #remove\n",
    "    CurrMet = SortedNodes[:top_n_nodes]\n",
    "    if 'biomass[c]' in CurrMet:\n",
    "        CurrMet = [i for i in CurrMet if i != 'biomass[c]']\n",
    "\n",
    "    curr_N = copy.deepcopy(glc_G)\n",
    "    curr_N.remove_nodes_from(CurrMet)\n",
    "\n",
    "    curr_adj = nx.to_pandas_adjacency(curr_N)\n",
    "    curr_ID = list(curr_adj.columns)\n",
    "\n",
    "    curr_Met_adjs.append(curr_adj)\n",
    "    curr_Met_IDs.append(curr_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export filtered adjacency networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{current_dir}/0_FilteredAdjacency', exist_ok=True)\n",
    "for m, fil_adj in enumerate(curr_Met_adjs):\n",
    "    fil_adj.to_csv(f'{current_dir}/0_FilteredAdjacency/{Species[m]}_FilteredNetwork.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find layers of the networks based on distance from the core - biomass product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rest = {}\n",
    "Neighbors = {}\n",
    "for number in range(len(Species)):\n",
    "    bio_prods = BiomassRxns[number][1]\n",
    "    bio_prods = [i for i in bio_prods if i in curr_Met_IDs[number]]\n",
    "    \n",
    "    last_G = nx.from_pandas_adjacency(curr_Met_adjs[number], create_using = nx.DiGraph)\n",
    "\n",
    "    slices = {}\n",
    "    target_comps = bio_prods\n",
    "    previous = target_comps\n",
    "    count = 0\n",
    "    while True:\n",
    "        neighbors = []\n",
    "        for target in target_comps:\n",
    "            particular_predecessors = [i for i in last_G.predecessors(target)]\n",
    "            neighbors.append(particular_predecessors)\n",
    "\n",
    "        new_neighbors = set([item for sublist in neighbors for item in sublist])        \n",
    "        target_comps = [i for i in new_neighbors if i not in previous]\n",
    "        if len(target_comps) == 0:\n",
    "            break\n",
    "\n",
    "        slices[f'{count+1}_neigbours'] = target_comps\n",
    "        previous += new_neighbors\n",
    "        count += 1\n",
    "\n",
    "    Neighbors[Species[number]] = slices\n",
    "    neighs = [item for sublist in list(slices.values()) for item in sublist]\n",
    "    Rest[Species[number]] = [q for q in curr_Met_IDs[number] if q not in neighs]\n",
    "\n",
    "\n",
    "comparableSlices = np.min([len(list(list(Neighbors.values())[i].values())) for i in range(len(Species))]) - 1\n",
    "Neighbors_layers = {}\n",
    "keyss = [f'D{i}' for i in range(1, comparableSlices+2)] + ['R']\n",
    "for sps in Species:\n",
    "    valuess = [Neighbors[sps][f'{i}_neigbours'] for i in range(1, comparableSlices+1)]\n",
    "    valuess.append([item for sublist in [Neighbors[sps][f'{j}_neigbours'] for j in range(comparableSlices+1, len(Neighbors[sps])+1)] for item in sublist])\n",
    "    valuess.append(Rest[sps])\n",
    "    inner_dict = dict(zip(keyss, valuess))\n",
    "    Neighbors_layers[sps] = inner_dict\n",
    "\n",
    "notation = [f'D{i+1}' for i in range(len(Neighbors_layers[sps])-1)]+['R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{current_dir}/1_SpeciesMetaLayers', exist_ok=True)\n",
    "for ll in notation:\n",
    "        layer_l1 = []\n",
    "        for spec in Species:\n",
    "            layer_l1.append(Neighbors_layers[spec][ll])\n",
    "        layer = pd.DataFrame(layer_l1, index=Species).T\n",
    "        layer.to_csv(f'1_SpeciesMetaLayers/Networklayered_{ll}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and export competitive and synergistic indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_s = [(i, i) for i in notation]\n",
    "SI_s = [i for i in itertools.permutations(notation, 2)]\n",
    "\n",
    "### CI ###\n",
    "os.makedirs(f'{current_dir}/2_Indices/CI', exist_ok=True)\n",
    "for notn1 in CI_s:\n",
    "    CI_Matrix = pd.DataFrame(np.zeros((len(Species), len(Species)), dtype=float), columns= Species)\n",
    "    CI_Matrix.index = CI_Matrix.columns\n",
    "\n",
    "    for speci1 in Species:\n",
    "        for speci2 in Species:\n",
    "            X1 = Neighbors_layers[speci1][notn1[0]]\n",
    "            X2 = Neighbors_layers[speci2][notn1[1]]\n",
    "\n",
    "            CI_XX = (len(list(set(X1) & set(X2))))/len(set(X1))\n",
    "            CI_Matrix.loc[speci1, speci2] = CI_XX\n",
    "    CI_Matrix.to_csv(f'{current_dir}/2_Indices/CI/Network_CI_{notn1[0]}{notn1[1]}_Curr{perc}.csv', header=True, index=True)\n",
    "\n",
    "\n",
    "### SI ###\n",
    "os.makedirs(f'{current_dir}/2_Indices/SI', exist_ok=True)\n",
    "for notn2 in SI_s:\n",
    "    SI_Matrix = pd.DataFrame(np.zeros((len(Species), len(Species)), dtype=float), columns= Species)\n",
    "    SI_Matrix.index = SI_Matrix.columns\n",
    "\n",
    "    for speci1 in Species:\n",
    "        for speci2 in Species:\n",
    "            X2 = Neighbors_layers[speci2][notn2[0]]\n",
    "            Y1 = Neighbors_layers[speci1][notn2[1]]\n",
    "\n",
    "            SI_XY = (len(list(set(X2) & set(Y1))))/len(set(Y1))\n",
    "            SI_Matrix.loc[speci1, speci2] = SI_XY\n",
    "    SI_Matrix.to_csv(f'{current_dir}/2_Indices/SI/Network_SI_{notn2[0]}{notn2[1]}_Curr{perc}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbiomE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb673bca86fdae5149ce9e03649aa8b5b46596c745ba9a374f65b2f7805672bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
